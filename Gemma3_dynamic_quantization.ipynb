{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96a77c6-e903-4b42-ba3c-f56511ff0bad",
   "metadata": {},
   "source": [
    "# Notebook Setup and Execution \n",
    "\n",
    "## Environment: \n",
    "- NVIDIA GH200 96GB GPU. If you are using H100, consider reducing batch size to 4.\n",
    "- The training of the model is very slow on the full data. We recommend using a sample data.\n",
    "- Moreover, the CPU inference of this will be equally slow slow, so GPU is recommended for both training and evaluation. \n",
    "- we provide the Jupyter notebook gemma3_dynamic_quant.ipynb. You may try to run the notebook as it is because we added the installation of the necessary libraries on top.\n",
    "\n",
    "## Below are instructions and requirements for setting up and running the notebook: \n",
    "###  Environment & Dependencies: \n",
    "- Python 3.9 or above, with PyTorch and TorchVision installed \n",
    "- Hugging Face Transformers library (version 4.x) to load the Gemma 3 model or you can get the model from the correct transformers from google source code added on top.\n",
    "- huggingface_hub library for model download is optional.\n",
    "- OpenCV for image processing used to compute Sobel edges for edge density. \n",
    "- SciPy for some image filtering and entropy calculation. \n",
    "- scikit-image for computing Local Binary Patterns. \n",
    "- scikit-learn for metrics ROC AUC and an optional stratified splitter \n",
    "- Pandas for data handling and reading csv. \n",
    "- Tqdm for progress bars. \n",
    "- Matplotlib for plotting results. \n",
    "\n",
    "### Data Preparation: \n",
    "- Download MIMIC-CXR-JPG images and labels. You need PhysioNet credentials to access the dataset (https://physionet.org/content/mimic-cxr-jpg/2.1.0/). \n",
    "- Update the notebook configuration paths: \n",
    "- image_dir should point to the root folder containing the JPG files. \n",
    "- filenames_path can point to a text file or CSV listing the image filenames you intend to use \n",
    "- metadata_path should point to the CSV file with labels. \n",
    "- Make sure you have sufficient disk space for the data and the model.\n",
    " \n",
    "### Running the Notebook: \n",
    "- Start the Jupyter environment and open gemma3_dynamic_quant.ipynb. \n",
    "- Step through the notebook cells in order. The notebook is organized into sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a1fff-26d9-43d1-a12c-b1817dae547c",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "This was trained on a H200 GPU on Lambda cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730aab2-b371-4d50-8f46-79f9e5b3c871",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9faac6-cb9a-4ff4-bdaf-6cb97dba5432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
      "  Cloning https://github.com/huggingface/transformers (to revision v4.49.0-Gemma-3) to /tmp/pip-req-build-is3c7j2q\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-is3c7j2q\n",
      "  Running command git checkout -q 367bab469b0ef32017e2a0a0a5dbac5d36002f03\n",
      "  Resolved https://github.com/huggingface/transformers to commit 367bab469b0ef32017e2a0a0a5dbac5d36002f03\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.21.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.50.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.50.0.dev0) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers==4.50.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers==4.50.0.dev0) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2024.3.1)\n",
      "/home/ubuntu/data\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-image in /home/ubuntu/.local/lib/python3.10/site-packages (0.25.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: pillow>=10.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (11.2.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (2025.3.30)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (1.15.2)\n",
      "Requirement already satisfied: packaging>=21 in /usr/lib/python3/dist-packages (from scikit-image) (21.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python-headless in /home/ubuntu/.local/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from opencv-python-headless) (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /home/ubuntu/.local/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-image in /home/ubuntu/.local/lib/python3.10/site-packages (0.25.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: pillow>=10.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (11.2.1)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (1.15.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (2025.3.30)\n",
      "Requirement already satisfied: packaging>=21 in /usr/lib/python3/dist-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-image) (3.4.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy<2 in /home/ubuntu/.local/lib/python3.10/site-packages (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: iterative-stratification in /home/ubuntu/.local/lib/python3.10/site-packages (0.1.9)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from iterative-stratification) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.10/site-packages (from iterative-stratification) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (from iterative-stratification) (0.23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
    "\n",
    "%cd /home/ubuntu/data\n",
    "\n",
    "!pip install scikit-image\n",
    "!pip install opencv-python-headless\n",
    "\n",
    "!pip install opencv-python\n",
    "!pip install scikit-image\n",
    "!pip install --upgrade pandas\n",
    "!pip install \"numpy<2\"\n",
    "!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18e61e-ce38-4fd8-a853-c4eca1f2e9bf",
   "metadata": {},
   "source": [
    "### AUTHENTICATE TO HUGGINGFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e470194-da15-430e-95b7-ce5cc18a132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "HF_TOKEN = \"\"\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d8803-28eb-46a2-8c21-1e94bb8daa9e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3427118b-b63a-4c2c-84ca-e3cacdead9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 23:44:29.094815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746747869.104161   28805 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746747869.109059   28805 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "import cv2\n",
    "from scipy import ndimage, stats\n",
    "from skimage import feature\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoTokenizer, Gemma3ForConditionalGeneration\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "import os, copy, gc, torch, random\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0b1bc-2a0c-4e37-9772-e1071e405352",
   "metadata": {},
   "source": [
    "### BASIC CONFIGRATION SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27217031-58e3-4015-b17b-0ee8d5701cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    filenames_path = \"/home/ubuntu/data/chex/IMAGE_FILENAMES_UPDATE\"\n",
    "    image_dir = \"/home/ubuntu/data/chex/mimic-cxr-jpg/2.1.0/\"\n",
    "    metadata_path = \"/home/ubuntu/data/chex/mimic-cxr-2.0.0-metadata.csv\"\n",
    "    chexpert_path = \"/home/ubuntu/data/chex/mimic-cxr-2.0.0-chexpert.csv\"\n",
    "    negbio_path = \"/home/ubuntu/data/chex/mimic-cxr-2.0.0-negbio.csv\"\n",
    "\n",
    "    batch_size = 8\n",
    "    img_size = 224\n",
    "    num_classes = 14\n",
    "    max_seq_length = 256\n",
    "\n",
    "    # I added the 3 configuration below for the uniform quantization\n",
    "    num_epochs = 1\n",
    "    lr = 1e-5\n",
    "    uniform_bits = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2a267-f178-4c54-a7fa-30a534469402",
   "metadata": {},
   "source": [
    "### MULTIMODAL MODEL DEFINITION WITH QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8631c13-fdb0-4deb-a803-890666b69b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalCheXpertModel(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        #  ViT from torchvision Image encoder \n",
    "        self.image_encoder = vit_b_16(pretrained=True)\n",
    "        self.image_encoder.heads = torch.nn.Identity()  # Remove classification head\n",
    "        self.image_proj = torch.nn.Linear(768, 512)\n",
    "\n",
    "        # using Gemma3ForConditionalGeneration fo Text encoding\n",
    "        self.text_encoder = Gemma3ForConditionalGeneration.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "        hidden_size_text = self.text_encoder.config.text_config.hidden_size\n",
    "        self.text_proj = torch.nn.Linear(hidden_size_text, 512)\n",
    "\n",
    "        # Classifier head for text features\n",
    "        self.classifier_head = torch.nn.Linear(512, config.num_classes)\n",
    "\n",
    "        # Multimodal fusion classifier\n",
    "        self.classifier = torch.nn.Linear(1024, config.num_classes)\n",
    "\n",
    "    def forward(self, inputs, quant_level=None):\n",
    "        img_features = self.image_encoder(inputs[\"image\"])\n",
    "        img_features = self.image_proj(img_features)\n",
    "\n",
    "        text_output = self.text_encoder(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        text_features = text_output.hidden_states[-1][:, 0]\n",
    "        projected_text_features = self.text_proj(text_features)\n",
    "\n",
    "        # Fuse modalities by concatenating image and text features \n",
    "        combined = torch.cat((img_features, projected_text_features), dim=1)\n",
    "        \n",
    "        # Quantized the features we combined above.\n",
    "        if quant_level is not None:\n",
    "            level_to_bits = {0: 1, 1: 2, 2: 3, 3: 4}\n",
    "            bitwidth = level_to_bits.get(quant_level, 4)\n",
    "            combined = quantize_tensor(combined, bitwidth)\n",
    "        \n",
    "        return self.classifier(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8aaef4-04dc-4542-a922-436c6ae8b59f",
   "metadata": {},
   "source": [
    "### DATASET DEFINITION AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f202f788-6f34-4bf7-9de3-70e8a0be8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CXRDataset(Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # Loading filenames & metadata\n",
    "        self.filenames = pd.read_csv(config.filenames_path, header=None, names=[\"filename\"])\n",
    "        self.filenames[\"dicom_id\"] = (\n",
    "            self.filenames[\"filename\"]\n",
    "            .str.split(\"/\")\n",
    "            .str[-1]\n",
    "            .str.split(\".\")\n",
    "            .str[0]\n",
    "        )\n",
    "        self.metadata = pd.read_csv(config.metadata_path)\n",
    "        self.metadata[\"dicom_id\"] = self.metadata[\"dicom_id\"].astype(str)\n",
    "\n",
    "        # Merging filenames and metadata\n",
    "        merged = self.filenames.merge(self.metadata, on=\"dicom_id\", how=\"inner\")\n",
    "        merged[\"subject_id\"] = pd.to_numeric(merged[\"subject_id\"])\n",
    "        merged[\"study_id\"]   = pd.to_numeric(merged[\"study_id\"])\n",
    "\n",
    "        # Merging CheXpert labels\n",
    "        self.chexpert = pd.read_csv(config.chexpert_path)\n",
    "        self.chexpert[[\"subject_id\",\"study_id\"]] = self.chexpert[[\"subject_id\",\"study_id\"]].apply(pd.to_numeric)\n",
    "        merged = merged.merge(self.chexpert, on=[\"subject_id\",\"study_id\"], how=\"inner\")\n",
    "\n",
    "        # Merging NegBio labels\n",
    "        self.negbio = pd.read_csv(config.negbio_path)\n",
    "        self.negbio[[\"subject_id\",\"study_id\"]] = self.negbio[[\"subject_id\",\"study_id\"]].apply(pd.to_numeric)\n",
    "        data = merged.merge(self.negbio, on=[\"subject_id\",\"study_id\"], suffixes=(\"\", \"_negbio\"), how=\"inner\")\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(\"Data merging failed: check DICOM IDs and label files.\")\n",
    "\n",
    "        # Identifying & clean multi‑label columns once\n",
    "        self.label_cols = [\n",
    "            col for col in self.chexpert.columns\n",
    "            if col not in (\"subject_id\", \"study_id\")\n",
    "        ]\n",
    "        data[self.label_cols] = (\n",
    "            data[self.label_cols]\n",
    "            .fillna(0)\n",
    "            .replace(-1, 0)\n",
    "        )\n",
    "\n",
    "        # Filtering out image files that are missing\n",
    "        data = data[data[\"filename\"].apply(\n",
    "            lambda fn: os.path.exists(os.path.join(config.image_dir, fn))\n",
    "        )]\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((config.img_size, config.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "        ])\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Coverting Images to RGB\n",
    "        img = Image.open(os.path.join(self.config.image_dir, row[\"filename\"])).convert(\"RGB\")\n",
    "        image = self.transform(img)\n",
    "\n",
    "        # Texting prompt and encoding\n",
    "        text_prompt = (\n",
    "            f\"Findings: {row['PerformedProcedureStepDescription']}  \"\n",
    "            f\"View: {row['ViewPosition']}  \"\n",
    "            f\"Orientation: {row.get('PatientOrientationCodeSequence_CodeMeaning','Unknown')}\"\n",
    "        )\n",
    "        enc = self.tokenizer(\n",
    "            text_prompt,\n",
    "            max_length=self.config.max_seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids      = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        labels = torch.FloatTensor(\n",
    "            row[self.label_cols].values.astype(float)\n",
    "        )\n",
    "\n",
    "        # text complexity\n",
    "        text_complexity = compute_text_entropy(text_prompt)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"text_prompt\": text_prompt,\n",
    "            \"text_complexity\": text_complexity\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e590ac7-0e2f-4b27-b076-f7e307e1afbd",
   "metadata": {},
   "source": [
    "## COMPLEXITY METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4b23a-2ae6-4383-8e7a-0e039d308c4c",
   "metadata": {},
   "source": [
    "### COMPLEXITY METRIC FUNCTIONS FOR IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a136bc30-b616-40a4-8dc3-0d79681adee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_shannon_entropy(pil_image):\n",
    "    gray = pil_image.convert(\"L\")\n",
    "    np_gray = np.array(gray)\n",
    "    histogram, _ = np.histogram(np_gray, bins=256, range=(0, 255))\n",
    "    prob = histogram / (histogram.sum() + 1e-7)\n",
    "    entropy_value = -np.sum(prob * np.log2(prob + 1e-7))\n",
    "    return float(entropy_value)\n",
    "\n",
    "def compute_edge_density(pil_image, threshold=20):\n",
    "    gray = np.array(pil_image.convert(\"L\"), dtype=np.float32)\n",
    "    dx = ndimage.sobel(gray, axis=0)\n",
    "    dy = ndimage.sobel(gray, axis=1)\n",
    "    mag = np.hypot(dx, dy)\n",
    "    edge_pixels = np.sum(mag > threshold)\n",
    "    density = edge_pixels / gray.size\n",
    "    return float(density)\n",
    "\n",
    "def compute_intensity_variation(pil_image):\n",
    "    gray = np.array(pil_image.convert(\"L\"), dtype=np.float32)\n",
    "    return float(np.std(gray))\n",
    "\n",
    "def compute_fractal_dimension(pil_image, threshold=128):\n",
    "    gray = np.array(pil_image.convert(\"L\"))\n",
    "    binary = gray < threshold\n",
    "\n",
    "    def boxcount(Z, k):\n",
    "        S = np.add.reduceat(\n",
    "            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
    "            np.arange(0, Z.shape[1], k), axis=1)\n",
    "        return np.sum((S > 0) & (S < k*k))\n",
    "\n",
    "    p = min(binary.shape)\n",
    "    n = 2**np.floor(np.log2(p))\n",
    "    n = int(n)\n",
    "    Z = binary[:n, :n]\n",
    "    sizes = 2**np.arange(int(np.log2(n)), 1, -1)\n",
    "    counts = [boxcount(Z, size) for size in sizes]\n",
    "    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
    "    return -coeffs[0]\n",
    "\n",
    "def compute_lbp_complexity(pil_image, radius=1, n_points=8):\n",
    "    gray = np.array(pil_image.convert(\"L\"))\n",
    "    lbp = feature.local_binary_pattern(gray, n_points, radius, method=\"uniform\")\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    s = hist.sum() + 1e-7\n",
    "    hist /= s\n",
    "    lbp_entropy = -np.sum(hist * np.log2(hist + 1e-7))\n",
    "    if not np.isfinite(lbp_entropy):\n",
    "        lbp_entropy = 0.0\n",
    "    return float(lbp_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42bb33b-07d0-4c34-9f31-4bfe822422d5",
   "metadata": {},
   "source": [
    "### COMPLEXITY METRIC FUNCTION FOR TEXT - entropy based on character frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9fd9c25-d7ee-4d8b-a2ae-482942474ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_text_entropy(text):\n",
    "    text = text.lower()\n",
    "    freq = {}\n",
    "    for char in text:\n",
    "        if char.isalnum():\n",
    "            freq[char] = freq.get(char, 0) + 1\n",
    "    total = sum(freq.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    entropy = -sum((count/total) * np.log2(count/total + 1e-7) for count in freq.values())\n",
    "    return float(entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf187ff-509b-41b6-8d87-9b370151ee7b",
   "metadata": {},
   "source": [
    "### DATA DESCRIPTION & STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b59053-776a-4637-95a6-2215ea8676d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28805/1315942256.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "full_ds = CXRDataset(config)\n",
    "\n",
    "sample_size = len(full_ds)\n",
    "indices = random.sample(range(len(full_ds)), sample_size)\n",
    "\n",
    "records = []\n",
    "for idx in indices:\n",
    "    row = full_ds.data.iloc[idx]\n",
    "    img_path = os.path.join(config.image_dir, row[\"filename\"])\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    sh = compute_shannon_entropy(image)\n",
    "    ed = compute_edge_density(image)\n",
    "    iv = compute_intensity_variation(image)\n",
    "    fd = compute_fractal_dimension(image)\n",
    "    lbp = compute_lbp_complexity(image)\n",
    "    \n",
    "    text_prompt = (\n",
    "        f\"Findings: {row['PerformedProcedureStepDescription']} \"\n",
    "        f\"View: {row['ViewPosition']} \"\n",
    "        f\"Orientation: {row.get('PatientOrientationCodeSequence_CodeMeaning','Unknown')}\"\n",
    "    )\n",
    "    te = compute_text_entropy(text_prompt)\n",
    "    \n",
    "    records.append({\n",
    "        \"shannon_entropy\": sh,\n",
    "        \"edge_density\": ed,\n",
    "        \"intensity_variation\": iv,\n",
    "        \"fractal_dimension\": fd,\n",
    "        \"lbp_entropy\": lbp,\n",
    "        \"text_entropy\": te\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db855786-3635-41de-a3f3-2a257657c1c5",
   "metadata": {},
   "source": [
    "#### Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75372fa-0a2f-412a-9814-dff09d885f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    plt.figure()\n",
    "    plt.hist(df[col], bins=30)\n",
    "    plt.title(f\"Distribution of {col.replace('_', ' ').title()}\")\n",
    "    plt.xlabel(col.replace('_', ' ').title())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9541fd-9626-4f69-baf8-61927b155493",
   "metadata": {},
   "source": [
    "#### Scatter: image vs text entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da5ac2-3e2d-4cde-9361-02b0966620e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(df[\"shannon_entropy\"], df[\"text_entropy\"])\n",
    "plt.title(\"Image Shannon Entropy vs Text Entropy\")\n",
    "plt.xlabel(\"Image Shannon Entropy\")\n",
    "plt.ylabel(\"Text Entropy\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc682aa-ca27-45cb-a997-5a00e5a68905",
   "metadata": {},
   "source": [
    "#### Label prevalence bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0ab28-ca99-49a0-8566-8d2373f5e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = full_ds.data[full_ds.label_cols].sum()\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.bar(label_counts.index, label_counts.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"CheXpert Label Prevalence\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90ae99-ec99-41ed-880a-a2d1efb36aa2",
   "metadata": {},
   "source": [
    "## QUANTIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cdd0fa-b6fc-4b28-8d30-3f6093303621",
   "metadata": {},
   "source": [
    "### QUANTIZATION FUNCTION (for scalar complexity values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53545ae-6669-4458-bbf3-312cb2831af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_complexity(value, min_val, max_val, levels=4):\n",
    "    if max_val == min_val:\n",
    "        return 0\n",
    "    step = (max_val - min_val) / levels\n",
    "    level = int((value - min_val) / step)\n",
    "    if level >= levels:\n",
    "        level = levels - 1\n",
    "    return level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef615d-e295-432a-833c-b211c4663e7a",
   "metadata": {},
   "source": [
    "### UNIFORM TENSOR QUANTIZATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d8473-587c-4efc-be9b-79103b9d0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor(tensor, num_bits):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    qmin = 0\n",
    "    qmax = 2**num_bits - 1\n",
    "    if max_val == min_val:\n",
    "        return tensor\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    tensor_q = torch.round((tensor - min_val) / scale)\n",
    "    tensor_dq = tensor_q * scale + min_val\n",
    "    return tensor_dq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d6a0d-96e8-44f7-b115-c3bc88f5982f",
   "metadata": {},
   "source": [
    "## MAIN SCRIPT: TRAINING, COMPLEXITY ANALYSIS, INFERENCE, AND COMPARISON GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699116bf-bb83-4e5b-af62-f518e4cd424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config  = Config()\n",
    "    full_ds = CXRDataset(config)\n",
    "    N = len(full_ds)\n",
    "\n",
    "    # print(full_ds.data.columns.tolist())\n",
    "  \n",
    "    label_cols = [\n",
    "        c for c in full_ds.chexpert.columns \n",
    "        if c not in (\"subject_id\", \"study_id\")\n",
    "    ]\n",
    "\n",
    "    # extracting multi‑label array for stratification\n",
    "    Y = full_ds.data[label_cols].values\n",
    "    \n",
    "    splitter = MultilabelStratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=0.10, random_state=42\n",
    "    )\n",
    "    train_idx, test_idx = next(splitter.split(X=np.zeros(N), y=Y))\n",
    "    \n",
    "    train_ds = torch.utils.data.Subset(full_ds, train_idx)\n",
    "    test_ds  = torch.utils.data.Subset(full_ds, test_idx)\n",
    "    \n",
    "    class_counts  = Y.sum(axis=0)\n",
    "    class_weights = 1.0 / (class_counts + 1e-6)\n",
    "    sample_weights = (Y * class_weights).sum(axis=1)\n",
    "    train_weights  = sample_weights[train_idx]\n",
    "\n",
    "    # We are making sure to use a stratified data\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=train_weights,\n",
    "        num_samples=len(train_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    train_loader    = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config.batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=4\n",
    "    )\n",
    "    val_loader      = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    analysis_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # initializing model, optimizer, loss \n",
    "    model     = MultimodalCheXpertModel(config).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training and Validation loops \n",
    "    num_epochs = 5\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses,   val_accs   = [], []\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        tl, ta = [], []\n",
    "        for batch in train_loader:\n",
    "            batch = {k:(v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "                     for k,v in batch.items()}\n",
    "            out   = model(batch)\n",
    "            loss  = criterion(out, batch[\"labels\"])\n",
    "            preds = (torch.sigmoid(out) > 0.5).float()\n",
    "            acc   = (preds == batch[\"labels\"]).float().mean().item()\n",
    "    \n",
    "            tl.append(loss.item()); ta.append(acc)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step(); optimizer.zero_grad()\n",
    "    \n",
    "        train_losses.append(np.mean(tl))\n",
    "        train_accs.append(np.mean(ta))\n",
    "    \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        vl, va = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k:(v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "                         for k,v in batch.items()}\n",
    "                out   = model(batch)\n",
    "                loss  = criterion(out, batch[\"labels\"])\n",
    "                preds = (torch.sigmoid(out) > 0.5).float()\n",
    "                acc   = (preds == batch[\"labels\"]).float().mean().item()\n",
    "    \n",
    "                vl.append(loss.item()); va.append(acc)\n",
    "    \n",
    "        val_losses.append(np.mean(vl))\n",
    "        val_accs.append(np.mean(va))\n",
    "    \n",
    "        print(\n",
    "            f\"Epoch {epoch} ▶ \"\n",
    "            f\"train_loss={train_losses[-1]:.4f}, train_acc={train_accs[-1]:.4f}  |  \"\n",
    "            f\"val_loss={val_losses[-1]:.4f},   val_acc={val_accs[-1]:.4f}\"\n",
    "        )\n",
    "    \n",
    "    # Plots\n",
    "    epochs = range(1, num_epochs+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train\")\n",
    "    plt.plot(epochs, val_losses,   label=\"Test\")\n",
    "    plt.title(\"Loss vs Epoch\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_accs, label=\"Train\")\n",
    "    plt.plot(epochs, val_accs,   label=\"Test\")\n",
    "    plt.title(\"Hamming Accuracy vs Epoch\"); plt.xlabel(\"Epoch\"); plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Complexity analysis on test split\n",
    "    unnorm = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std =[1/0.229,    1/0.224,    1/0.225]\n",
    "    )\n",
    "    \n",
    "    img_ent, txt_ent, combined = [], [], []\n",
    "    for sample in tqdm(analysis_loader, desc=\"Computing Complexity\"):\n",
    "        img_t = sample[\"image\"].squeeze(0)\n",
    "        img_u = unnorm(img_t).clamp(0,1)\n",
    "        img_p = transforms.ToPILImage()(img_u)\n",
    "        ie    = compute_shannon_entropy(img_p)\n",
    "        img_ent.append(ie)\n",
    "    \n",
    "        # Text entropy\n",
    "        te = sample[\"text_complexity\"]\n",
    "        te = te.item() if torch.is_tensor(te) else te\n",
    "        txt_ent.append(te)\n",
    "    \n",
    "        combined.append((ie + te) / 2)\n",
    "    \n",
    "    mn, mx = min(combined), max(combined)\n",
    "    quant_levels = [\n",
    "        quantize_complexity(x, mn, mx, levels=4) for x in combined\n",
    "    ]\n",
    "    \n",
    "    # Inference metrics on test split\n",
    "    model.eval()\n",
    "    y_true, p_full, p_q = [], [], []\n",
    "    reg_times, q_times = [], []\n",
    "    reg_accs, q_accs = [], []\n",
    "    errors_by_level = {i: [] for i in range(4)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample, lvl in zip(analysis_loader, quant_levels):\n",
    "            batch = {k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "                     for k, v in sample.items()}\n",
    "            y = batch[\"labels\"]\n",
    "\n",
    "            # 32 bits evaluation\n",
    "            torch.cuda.synchronize(); t0 = time.time()\n",
    "            o0 = model(batch, quant_level=None)\n",
    "            torch.cuda.synchronize(); reg_times.append(time.time() - t0)\n",
    "\n",
    "            # Quantized model evaluation\n",
    "            torch.cuda.synchronize(); t1 = time.time()\n",
    "            o1 = model(batch, quant_level=lvl)\n",
    "            torch.cuda.synchronize(); q_times.append(time.time() - t1)\n",
    "\n",
    "            p0 = torch.sigmoid(o0).cpu().numpy()\n",
    "            p1 = torch.sigmoid(o1).cpu().numpy()\n",
    "            y_true.append(y.cpu().numpy()); p_full.append(p0); p_q.append(p1)\n",
    "\n",
    "            pr0 = (p0 > 0.5).astype(int); pr1 = (p1 > 0.5).astype(int)\n",
    "            reg_accs.append((pr0 == y.cpu().numpy()).mean())\n",
    "            q_accs.append((pr1 == y.cpu().numpy()).mean())\n",
    "\n",
    "            err = np.abs(o0.cpu().numpy() - o1.cpu().numpy()).mean()\n",
    "            errors_by_level[lvl].append(err)\n",
    "\n",
    "    #  per class ROC‑AUC\n",
    "    Y  = np.vstack(y_true)\n",
    "    PF = np.vstack(p_full)\n",
    "    PQ = np.vstack(p_q)\n",
    "\n",
    "    auc_full  = np.full(len(label_cols), np.nan, dtype=float)\n",
    "    auc_quant = np.full(len(label_cols), np.nan, dtype=float)\n",
    "\n",
    "    for i in range(len(label_cols)):\n",
    "        col_true = Y[:, i]\n",
    "        if len(np.unique(col_true)) == 2:\n",
    "            auc_full[i]  = roc_auc_score(col_true, PF[:, i])\n",
    "            auc_quant[i] = roc_auc_score(col_true, PQ[:, i])\n",
    "\n",
    "    # we calculate the mean‑5 and macro‑14 and ignoring NaNs so we can compare the result with other papers, although those papers also included question/answering featres\n",
    "    chex5 = [\"Atelectasis\",\"Cardiomegaly\",\"Consolidation\",\"Edema\",\"Pleural Effusion\"]\n",
    "    idx5  = [label_cols.index(l) for l in chex5]\n",
    "    mean5_full  = np.nanmean(auc_full[idx5])\n",
    "    mean5_quant = np.nanmean(auc_quant[idx5])\n",
    "    macro_full  = np.nanmean(auc_full)\n",
    "    macro_quant = np.nanmean(auc_quant)\n",
    "\n",
    "    print(f\"Mean‑5 AUROC:  full={mean5_full:.3f}, quant={mean5_quant:.3f}\")\n",
    "    print(f\"Macro‑14 AUROC: full={macro_full:.3f}, quant={macro_quant:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180a8ff-d1f2-478f-ac86-da8f505ad7b7",
   "metadata": {},
   "source": [
    "## Model‑size & weight‑dtype comparison: FP32/BF16 vs INT8‑weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f10949-8664-473d-9f7a-227f1fbb8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_stats(model, name=\"model\"):\n",
    "    n_params     = sum(p.numel() for p in model.parameters())\n",
    "    n_trainable  = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    weight_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    return {\n",
    "        \"name\"       : name,\n",
    "        \"params (M)\" : f\"{n_params/1e6:7.2f}\",\n",
    "        \"trainable M\": f\"{n_trainable/1e6:7.2f}\",\n",
    "        \"mem (MB)\"   : f\"{weight_bytes/1024**2:8.1f}\",\n",
    "    }\n",
    "\n",
    "def print_table(*rows):\n",
    "    cols = list(rows[0].keys())\n",
    "    widths = {c: max(len(c), *(len(str(r[c])) for r in rows)) for c in cols}\n",
    "    fmt = \"  \".join(f\"{{:{w}}}\" for w in widths.values())\n",
    "    print(fmt.format(*cols))\n",
    "    print(fmt.format(*(\"-\"*w for w in widths.values())))\n",
    "    for r in rows:\n",
    "        print(fmt.format(*(r[c] for c in cols)))\n",
    "    print()\n",
    "\n",
    "fp_stats = model_stats(model, \"Full‑precision\")\n",
    "\n",
    "print(\"\\nCreating int8 dynamic‑quantised copy\")\n",
    "quantised_model = quantize_dynamic(\n",
    "    copy.deepcopy(model).cpu(), \n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "int8_stats = model_stats(quantised_model, \"INT8‑weights\")\n",
    "\n",
    "ckpt_fp = \"checkpoint_full.pt\"\n",
    "ckpt_i8 = \"checkpoint_int8.pt\"\n",
    "torch.save(model.state_dict(),      ckpt_fp)\n",
    "torch.save(quantised_model.state_dict(), ckpt_i8)\n",
    "\n",
    "for p in (ckpt_fp, ckpt_i8):\n",
    "    size_mb = os.path.getsize(p)/(1024**2)\n",
    "    print(f\"{p:<20} {size_mb:8.1f} MB\")\n",
    "\n",
    "print(\"\\n===  Model‑size comparison  ===\")\n",
    "print_table(fp_stats, int8_stats)\n",
    "\n",
    "# freeing CPU RAM used by the quantised copy for the next training\n",
    "del quantised_model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df368a-25e3-4aba-876f-9dbe9150e9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13d1bb-dcb3-417b-914a-eaf09ad3a7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f9ad76e-12a9-4558-8db5-25d5b6b1d5d7",
   "metadata": {},
   "source": [
    "# 2 bits quant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f1eff-535d-42fd-9381-4d55ca093ea1",
   "metadata": {},
   "source": [
    "### REDEFINING CONFIG FOR UNIFORM QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762afdba-e7fb-4217-9f0d-f4411a006c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    filenames_path = \"/home/ubuntu/data/chex/IMAGE_FILENAMES_UPDATE\"\n",
    "    image_dir = \"/home/ubuntu/data/chex/mimic-cxr-jpg/2.1.0/\"\n",
    "    metadata_path = \"/home/ubuntu/data/chex/mimic-cxr-2.0.0-metadata.csv\"\n",
    "    chexpert_path = \"/home/ubuntu/data/chex/mimic-cxr-2.0.0-chexpert.csv\"\n",
    "    negbio_path = \"/home/ubuntu/data/chex/mimic-cxr-2.0.0-negbio.csv\"\n",
    "\n",
    "    batch_size = 4\n",
    "    img_size = 224\n",
    "    num_classes = 14\n",
    "    max_seq_length = 256\n",
    "\n",
    "    # I added the 3 configuration below for the uniform quantization\n",
    "    num_epochs = 1\n",
    "    lr = 1e-5\n",
    "    uniform_bits = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5fc2d-f788-403d-881f-882f48f81826",
   "metadata": {},
   "source": [
    "## Uniform Quantization\n",
    "Uniformly quantize a float tensor to num_bits and dequantize in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fed154-9338-4a0f-8df9-1d35ebb67769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor(tensor: torch.Tensor, num_bits: int) -> torch.Tensor:\n",
    "    mn, mx = tensor.min(), tensor.max()\n",
    "    if mx == mn:\n",
    "        return tensor\n",
    "    qmin, qmax = 0, 2**num_bits - 1\n",
    "    scale = (mx - mn) / (qmax - qmin)\n",
    "    q = ( (tensor - mn) / scale ).round().clamp(qmin, qmax)\n",
    "    return q * scale + mn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d41cb-fa2d-4c35-81f3-3b9061ed0757",
   "metadata": {},
   "source": [
    "## CHEXPERT MML SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af9462-37b8-4f0c-9fdb-629e0ccc5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalCheXpertModel(torch.nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        # encoding images, removed head, encde text and add classifier of our configuration\n",
    "        self.img_enc  = vit_b_16(pretrained=True)\n",
    "        self.img_enc.heads = torch.nn.Identity()\n",
    "        self.img_proj = torch.nn.Linear(768,512)\n",
    "        self.txt_enc  = Gemma3ForConditionalGeneration.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "        hs = self.txt_enc.config.text_config.hidden_size\n",
    "        self.txt_proj  = torch.nn.Linear(hs,512)\n",
    "        self.classifier = torch.nn.Linear(1024, cfg.num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        xi = self.img_enc(batch[\"image\"])\n",
    "        xi = self.img_proj(xi)\n",
    "        to = self.txt_enc(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            output_hidden_states=True, return_dict=True\n",
    "        )\n",
    "        xt = to.hidden_states[-1][:,0]\n",
    "        xt = self.txt_proj(xt)\n",
    "        fused = torch.cat([xi, xt], dim=1)\n",
    "        return self.classifier(fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f639c-eb0f-4070-9ef0-551e39cfe601",
   "metadata": {},
   "source": [
    "## Metrics and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68f626-37db-4ef9-bdba-820aa9022bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_stats(model, name:str):\n",
    "    ps = list(model.parameters())\n",
    "    total = sum(p.numel() for p in ps)\n",
    "    train = sum(p.numel() for p in ps if p.requires_grad)\n",
    "    mem   = sum(p.numel()*p.element_size() for p in ps)/1024**2\n",
    "    return {\"name\":name,\n",
    "            \"params (M)\":f\"{total/1e6:7.2f}\",\n",
    "            \"trainable M\":f\"{train/1e6:7.2f}\",\n",
    "            \"mem (MB)\":f\"{mem:8.1f}\"}\n",
    "\n",
    "def print_table(*rows):\n",
    "    keys   = list(rows[0].keys())\n",
    "    widths = {k:max(len(k),*(len(str(r[k])) for r in rows)) for k in keys}\n",
    "    fmt    = \"  \".join(f\"{{:{widths[k]}}}\" for k in keys)\n",
    "    print(fmt.format(*keys))\n",
    "    print(fmt.format(*(\"-\"*widths[k] for k in keys)))\n",
    "    for r in rows: print(fmt.format(*(r[k] for k in keys)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b261ee4-1c0b-4a45-96e2-f10e4f1c8a58",
   "metadata": {},
   "source": [
    "## Main class for uniform quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ce35a-3869-4330-ada9-724d0506d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cfg    = Config()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ds = CXRDataset(cfg)\n",
    "    dl = DataLoader(ds, batch_size=cfg.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # defining model, opt, loss\n",
    "    model     = MultimodalCheXpertModel(cfg).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model.train()\n",
    "    for batch in tqdm(dl, desc=\"Training Epoch\"):\n",
    "        batch = {k:(v.to(device) if isinstance(v,torch.Tensor) else v)\n",
    "                 for k,v in batch.items()}\n",
    "        out  = model(batch)\n",
    "        loss = criterion(out, batch[\"labels\"])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step(); optimizer.zero_grad()\n",
    "\n",
    "    # We can move the model to the cpu\n",
    "    model_cpu = copy.deepcopy(model).cpu()\n",
    "    for _, p in model_cpu.named_parameters():\n",
    "        with torch.no_grad():\n",
    "            p.data = torch.nan_to_num(p.data,\n",
    "                                      nan=0.0,\n",
    "                                      posinf=torch.finfo(p.dtype).max,\n",
    "                                      neginf=torch.finfo(p.dtype).min)\n",
    "\n",
    "    #  and display statistics of full precision \n",
    "    fp_stats = model_stats(model_cpu, \"Full-precision\")\n",
    "    torch.save(model_cpu.state_dict(), \"checkpoint_fp32.pt\")\n",
    "\n",
    "    # as well as the static uniform quant.\n",
    "    model_uq = copy.deepcopy(model_cpu)\n",
    "    B = cfg.uniform_bits\n",
    "    for m in model_uq.modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            with torch.no_grad():\n",
    "                m.weight.data = quantize_tensor(m.weight.data, B)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data   = quantize_tensor(m.bias.data, B)\n",
    "\n",
    "    uq_stats = model_stats(model_uq, f\"Uniform-{B}bit\")\n",
    "    torch.save(model_uq.state_dict(), f\"checkpoint_uq{B}.pt\")\n",
    "\n",
    "    # then we can do a size comparison\n",
    "    print(\"\\n=== Model-size comparison ===\")\n",
    "    print_table(fp_stats, uq_stats)\n",
    "\n",
    "    #  by displaying the CPU evaluation metrics. Note that this take a longtime\n",
    "    model_cpu.eval(); model_uq.eval()\n",
    "    y_true, p_fp, p_uq = [], [], []\n",
    "    h_fp, h_uq, mae_e = [], [], []\n",
    "\n",
    "    for batch in tqdm(dl, desc=\"Final eval\"):\n",
    "        batch = {k:(v.cpu() if isinstance(v,torch.Tensor) else v)\n",
    "                 for k,v in batch.items()}\n",
    "        Y    = batch[\"labels\"].numpy()\n",
    "        o_fp = model_cpu(batch)\n",
    "        o_u  = model_uq(batch)\n",
    "    \n",
    "        prob_fp = torch.sigmoid(o_fp).detach().numpy()\n",
    "        prob_uq = torch.sigmoid(o_u).detach().numpy()\n",
    "    \n",
    "        y_true.append(Y)\n",
    "        p_fp.append(prob_fp)\n",
    "        p_uq.append(prob_uq)\n",
    "    \n",
    "        pred_fp = (prob_fp > 0.5).astype(int)\n",
    "        pred_uq = (prob_uq > 0.5).astype(int)\n",
    "        h_fp.append((pred_fp == Y).mean())\n",
    "        h_uq.append((pred_uq == Y).mean())\n",
    "    \n",
    "        mae_e.append(np.abs(o_fp.detach().numpy() - o_u.detach().numpy()).mean())\n",
    "\n",
    "    # Stack and compute AUC\n",
    "    Y_all = np.vstack(y_true)\n",
    "    PF    = np.vstack(p_fp)\n",
    "    PU    = np.vstack(p_uq)\n",
    "    auc_fp = roc_auc_score(Y_all, PF, average=None)\n",
    "    auc_u  = roc_auc_score(Y_all, PU, average=None)\n",
    "\n",
    "    core5    = [ ds.label_cols.index(l) for l in\n",
    "                 [\"Atelectasis\",\"Cardiomegaly\",\"Consolidation\",\"Edema\",\"Pleural Effusion\"] ]\n",
    "    mean5fp  = auc_fp[core5].mean()\n",
    "    mean5uq  = auc_u[ core5].mean()\n",
    "    macro_fp = auc_fp.mean()\n",
    "    macro_u  = auc_u.mean()\n",
    "    h_fp_all = np.mean(h_fp)\n",
    "    h_uq_all = np.mean(h_uq)\n",
    "    mae_all  = np.mean(mae_e)\n",
    "\n",
    "    print(\"\\n=== Final metrics ===\")\n",
    "    print(f\"Hamming-Acc    | FP32 = {h_fp_all:.4f},  UQ = {h_uq_all:.4f}\")\n",
    "    print(f\"Mean-5  AUROC  | FP32 = {mean5fp:.4f},     UQ = {mean5uq:.4f}\")\n",
    "    print(f\"Macro-14 AUROC | FP32 = {macro_fp:.4f},     UQ = {macro_u:.4f}\")\n",
    "    print(f\"Logit-MAE      |                    {mae_all:.4f}\")\n",
    "\n",
    "    # cleaning the model from the CPU\n",
    "    del model_cpu, model_uq\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587316f-ae0e-41c8-8172-947ebc243414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
